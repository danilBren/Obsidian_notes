**Cross Entropy**: measure of the difference between two probability distributions: the true distribution and an estimated model distribution.
**How to**:
1. $-\log(Predicted_p)$ 
Cross entropy is nice as the the loss is **high** when the network makes a bad prediction. The slope of the line is high, so the step that will be taken is large. 

![[Pasted image 20241101215954.png]]

To find a **good classifier with a small VC-dimension** (meaning a model with good generalization and low complexity),
